{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN7HWxB1q2sODkKr1bDBgQf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h-Lxj20CQS4Y","executionInfo":{"status":"ok","timestamp":1754817235419,"user_tz":-540,"elapsed":28392,"user":{"displayName":"geumjin99","userId":"13917999733299572466"}},"outputId":"1f3f25ba-e299-42cb-b42d-b92ed00d9ba0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import glob\n","import os\n","import json\n","from tqdm import tqdm\n","\n","# ========== Paths (fill yours) ==========\n","BASE_FILE = \"/content/drive/Othercomputers/我的 Mac/pm_stats/base_stations.csv\"\n","DATA_FOLDER = \"/content/drive/Othercomputers/我的 Mac/pm_stats/2024S-Dot\"\n","OUTPUT_JSON = \"/content/drive/Othercomputers/我的 Mac/pm_stats/results/summary.json\"\n","\n","# ========== Controls ==========\n","DATE_START = \"2024-01-01\"\n","DATE_END   = \"2024-01-31\"\n","\n","# Start AFTER this serial in base (None = process all)\n","START_AFTER_SERIAL = None  # e.g. \"OC3CL200102\" or None\n","\n","# Select metrics to process: \"ALL\" or comma-separated keys, e.g. \"TEMP,NO2,CO\"\n","SELECTED_METRICS_STR = \"TEMP\"  # change as needed\n","\n","# ========== Metric dictionary (key -> (max_col, mean_col, min_col)) ==========\n","METRIC_MAP = {\n","    # Pollutants\n","    \"NO2\": (\"이산화질소 최대(ppm)\", \"이산화질소 평균(ppm)\", \"이산화질소 최소(ppm)\"),\n","    \"CO\":  (\"일산화탄소 최대(ppm)\", \"일산화탄소 평균(ppm)\", \"일산화탄소 최소(ppm)\"),\n","    \"SO2\": (\"이산화황 최대(ppm)\", \"이산화황 평균(ppm)\", \"이산화황 최소(ppm)\"),\n","    \"NH3\": (\"암모니아 최대(ppm)\", \"암모니아 평균(ppm)\", \"암모니아 최소(ppm)\"),\n","    \"H2S\": (\"황화수소 최대(ppm)\", \"황화수소 평균(ppm)\", \"황화수소 최소(ppm)\"),\n","    \"O3\":  (\"오존 최대(ppm)\",     \"오존 평균(ppm)\",     \"오존 최소(ppm)\"),\n","    # (Optional) environmental metrics, keep commented unless explicitly needed\n","     \"TEMP\": (\"온도 최대(℃)\", \"온도 평균(℃)\", \"온도 최소(℃)\"),\n","     \"HUM\":  (\"습도 최대(%)\", \"습도 평균(%)\", \"습도 최소(%)\"),\n","}\n","\n","# Wind columns to attach at the timestamps of max/min if present\n","WIND_COLS = [\"풍속 최대(m/s)\", \"풍속 평균(m/s)\", \"풍속 최소(m/s)\",\n","             \"풍향 최대(m/s)\", \"풍향 평균(m/s)\", \"풍향 최소(m/s)\"]"],"metadata":{"id":"27LQLlDpRRfW","executionInfo":{"status":"ok","timestamp":1754818367945,"user_tz":-540,"elapsed":4,"user":{"displayName":"geumjin99","userId":"13917999733299572466"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# ========== Helpers ==========\n","def clean_header(df):\n","    \"\"\"去掉表头空格并处理重复列名\"\"\"\n","    cols = df.columns.str.strip()\n","    seen = {}\n","    new_cols = []\n","    for col in cols:\n","        if col in seen:\n","            seen[col] += 1\n","            new_cols.append(f\"{col}.{seen[col]}\")\n","        else:\n","            seen[col] = 0\n","            new_cols.append(col)\n","    df.columns = new_cols\n","    return df\n","\n","def read_csv_safe(path: str) -> pd.DataFrame:\n","    \"\"\"Robust CSV reader handling utf-8-sig/cp949 and bad lines; cleans header.\"\"\"\n","    try:\n","        df = pd.read_csv(path, encoding=\"utf-8-sig\", low_memory=False, index_col=False, on_bad_lines=\"skip\")\n","    except UnicodeDecodeError:\n","        df = pd.read_csv(path, encoding=\"cp949\", low_memory=False, index_col=False, on_bad_lines=\"skip\")\n","    return clean_header(df)\n","\n","def resolve_col(cols_present, base_name: str):\n","    \"\"\"Resolve a column by exact match; if not found, try startswith (handles '.1' duplicates).\"\"\"\n","    if base_name in cols_present:\n","        return base_name\n","    cands = [c for c in cols_present if c.startswith(base_name)]\n","    return cands[0] if cands else None\n","\n","def parse_selected(selected_str: str):\n","    if selected_str.strip().upper() == \"ALL\":\n","        return list(METRIC_MAP.keys())\n","    items = [s.strip().upper() for s in selected_str.split(\",\") if s.strip()]\n","    return [k for k in items if k in METRIC_MAP]\n","\n","# ========== Load base and station list ==========\n","base_df = pd.read_csv(BASE_FILE, encoding=\"utf-8-sig\")\n","start_idx = 0\n","if START_AFTER_SERIAL is not None and START_AFTER_SERIAL in base_df[\"시리얼\"].values:\n","    start_idx = base_df.index[base_df[\"시리얼\"] == START_AFTER_SERIAL][0] + 1\n","work_base = base_df.iloc[start_idx:].copy()\n","\n","# Station meta and set\n","stations = []\n","station_info_map = {}\n","for _, r in work_base.iterrows():\n","    serial = r[\"시리얼\"]\n","    stations.append(serial)\n","    station_info_map[serial] = {\n","        \"모델번호\": r.get(\"모델번호\"),\n","        \"시리얼\": r.get(\"시리얼\"),\n","        \"지역\": r.get(\"지역\"),\n","        \"자치구\": r.get(\"자치구\"),\n","        \"행정동\": r.get(\"행정동\"),\n","    }\n","station_set = set(stations)\n","\n","# Date range scaffold\n","date_index = pd.date_range(DATE_START, DATE_END, freq=\"D\")\n","date_strs = [d.strftime(\"%Y-%m-%d\") for d in date_index]\n","\n","# Which metrics\n","selected_keys = parse_selected(SELECTED_METRICS_STR)\n","if not selected_keys:\n","    raise ValueError(\"No valid metrics selected. Check SELECTED_METRICS_STR and METRIC_MAP keys.\")\n","metrics_to_process = {k: METRIC_MAP[k] for k in selected_keys}\n","\n","# ========== Aggregation cache (incremental) ==========\n","# agg[serial][date][metric] = {\n","#   'count': int,\n","#   'sum_mean': float,\n","#   'n_mean': int,\n","#   'max_val': float or None,\n","#   'max_time': str or None,\n","#   'max_winds': {wc: val or None},\n","#   'min_val': float or None,\n","#   'min_time': str or None,\n","#   'min_winds': {wc: val or None}\n","# }\n","agg = {s: {} for s in stations}\n","\n","def ensure_day_metric(serial, day_str, metric):\n","    if day_str not in agg[serial]:\n","        agg[serial][day_str] = {}\n","    if metric not in agg[serial][day_str]:\n","        agg[serial][day_str][metric] = {\n","            'count': 0,\n","            'sum_mean': 0.0,\n","            'n_mean': 0,\n","            'max_val': None, 'max_time': None, 'max_winds': {},\n","            'min_val': None, 'min_time': None, 'min_winds': {},\n","        }\n","\n","# ========== Pre-init final_result with station_info and empty 'data' ==========\n","final_result = {s: {\"station_info\": station_info_map[s], \"data\": {}} for s in stations}\n","\n","# ========== Iterate files (outer), update agg (inner) ==========\n","weekly_files = sorted(glob.glob(os.path.join(DATA_FOLDER, \"*.csv\")))\n","if not weekly_files:\n","    raise FileNotFoundError(\"No weekly CSV files found in DATA_FOLDER.\")\n","\n","for fp in tqdm(weekly_files, desc=\"Files\"):\n","    dfw = read_csv_safe(fp)\n","    if \"시리얼\" not in dfw.columns or \"측정시간\" not in dfw.columns:\n","        continue\n","\n","    # Keep only stations we care about\n","    dfw = dfw[dfw[\"시리얼\"].isin(station_set)]\n","    if dfw.empty:\n","        continue\n","\n","    # Build date column (YYYY-MM-DD)\n","    dfw[\"__date__\"] = pd.to_datetime(dfw[\"측정시간\"].astype(str).str[:10], errors=\"coerce\")\n","    dfw = dfw[dfw[\"__date__\"].notna()]\n","    dfw = dfw[(dfw[\"__date__\"] >= pd.to_datetime(DATE_START)) & (dfw[\"__date__\"] <= pd.to_datetime(DATE_END))]\n","    if dfw.empty:\n","        continue\n","\n","    # For each metric, compute group aggregates on the fly and update agg\n","    for key, (max_col, mean_col, min_col) in metrics_to_process.items():\n","        cols_present = dfw.columns\n","        r_max  = resolve_col(cols_present, max_col)\n","        r_mean = resolve_col(cols_present, mean_col)\n","        r_min  = resolve_col(cols_present, min_col)\n","\n","        # If metric columns are completely missing in this file, skip\n","        if r_max is None and r_mean is None and r_min is None:\n","            continue\n","\n","        s_max  = pd.to_numeric(dfw[r_max],  errors=\"coerce\") if r_max  else pd.Series([pd.NA]*len(dfw), index=dfw.index)\n","        s_mean = pd.to_numeric(dfw[r_mean], errors=\"coerce\") if r_mean else pd.Series([pd.NA]*len(dfw), index=dfw.index)\n","        s_min  = pd.to_numeric(dfw[r_min],  errors=\"coerce\") if r_min  else pd.Series([pd.NA]*len(dfw), index=dfw.index)\n","\n","        # Count = number of rows where ANY of the 3 are present\n","        any_vals = (s_max.notna() | s_mean.notna() | s_min.notna())\n","\n","        # --- Group-level sums ---\n","        grp = dfw.groupby([\"시리얼\", \"__date__\"], sort=False)\n","\n","        # counts\n","        count_series = any_vals.groupby([dfw[\"시리얼\"], dfw[\"__date__\"]]).sum()\n","\n","        # mean sum and n\n","        mean_sum = s_mean.where(s_mean.notna(), 0.0).groupby([dfw[\"시리얼\"], dfw[\"__date__\"]]).sum()\n","        mean_n   = s_mean.notna().groupby([dfw[\"시리얼\"], dfw[\"__date__\"]]).sum()\n","\n","        # max/min values\n","        max_val = s_max.groupby([dfw[\"시리얼\"], dfw[\"__date__\"]]).max(min_count=1)\n","        min_val = s_min.groupby([dfw[\"시리얼\"], dfw[\"__date__\"]]).min()\n","\n","        # --- First-occurrence rows of group max/min (robust across pandas versions) ---\n","        # Build helper frame with needed cols\n","        cols_for_pick = [\"시리얼\", \"__date__\", \"측정시간\"] + [wc for wc in WIND_COLS if wc in dfw.columns]\n","        pick = dfw[cols_for_pick].copy()\n","\n","        # Group keys\n","        gkeys = [dfw[\"시리얼\"], dfw[\"__date__\"]]\n","\n","        # idx of first non-null max/min per group (NaN if group is all NaN)\n","        idx_max = s_max.groupby(gkeys).idxmax()\n","        idx_min = s_min.groupby(gkeys).idxmin()\n","\n","        # Drop groups with no valid extrema\n","        idx_max = idx_max.dropna().astype(int)\n","        idx_min = idx_min.dropna().astype(int)\n","\n","        # Lookups for (serial, day_str) -> value / time / winds\n","        max_val_dict = {}\n","        min_val_dict = {}\n","        max_row_lookup = {}\n","        min_row_lookup = {}\n","\n","        # Fill max dicts\n","        for (serial_k, day_k), i in idx_max.items():\n","            val = s_max.iloc[i]\n","            day_str = pd.to_datetime(day_k).strftime(\"%Y-%m-%d\")\n","            max_val_dict[(serial_k, day_str)] = float(val) if pd.notna(val) else None\n","\n","            row = pick.iloc[i]\n","            info = {\"time\": row[\"측정시간\"]}\n","            for wc in WIND_COLS:\n","                if wc in pick.columns:\n","                    info[wc] = row.get(wc, None)\n","            max_row_lookup[(serial_k, day_str)] = info\n","\n","        # Fill min dicts\n","        for (serial_k, day_k), i in idx_min.items():\n","            val = s_min.iloc[i]\n","            day_str = pd.to_datetime(day_k).strftime(\"%Y-%m-%d\")\n","            min_val_dict[(serial_k, day_str)] = float(val) if pd.notna(val) else None\n","\n","            row = pick.iloc[i]\n","            info = {\"time\": row[\"측정시간\"]}\n","            for wc in WIND_COLS:\n","                if wc in pick.columns:\n","                    info[wc] = row.get(wc, None)\n","            min_row_lookup[(serial_k, day_str)] = info\n","\n","\n","\n","        # --- Update agg cache ---\n","        for (serial, day), c in count_series.items():\n","            day_str = pd.to_datetime(day).strftime(\"%Y-%m-%d\")\n","            if serial not in station_set:\n","                continue\n","            if day_str not in date_strs:\n","                continue\n","            ensure_day_metric(serial, day_str, key)\n","\n","            agg_item = agg[serial][day_str][key]\n","            agg_item['count'] += int(c)\n","\n","        for (serial, day), s in mean_sum.items():\n","            day_str = pd.to_datetime(day).strftime(\"%Y-%m-%d\")\n","            if serial not in station_set or day_str not in date_strs:\n","                continue\n","            ensure_day_metric(serial, day_str, key)\n","            agg[serial][day_str][key]['sum_mean'] += float(s)\n","\n","        for (serial, day), n in mean_n.items():\n","            day_str = pd.to_datetime(day).strftime(\"%Y-%m-%d\")\n","            if serial not in station_set or day_str not in date_strs:\n","                continue\n","            ensure_day_metric(serial, day_str, key)\n","            agg[serial][day_str][key]['n_mean'] += int(n)\n","\n","        for (serial, day_str), v in max_val_dict.items():\n","            if serial not in station_set or day_str not in date_strs:\n","                continue\n","            ensure_day_metric(serial, day_str, key)\n","            cur = agg[serial][day_str][key]\n","            # Update only if new max is greater\n","            if v is not None and (cur['max_val'] is None or v > cur['max_val']):\n","                cur['max_val'] = v\n","                info = max_row_lookup.get((serial, day_str), {})\n","                cur['max_time'] = info.get(\"time\")\n","                cur['max_winds'] = {wc: info.get(wc) for wc in WIND_COLS}\n","\n","        for (serial, day_str), v in min_val_dict.items():\n","            if serial not in station_set or day_str not in date_strs:\n","                continue\n","            ensure_day_metric(serial, day_str, key)\n","            cur = agg[serial][day_str][key]\n","            # Update only if new min is smaller\n","            if v is not None and (cur['min_val'] is None or v < cur['min_val']):\n","                cur['min_val'] = v\n","                info = min_row_lookup.get((serial, day_str), {})\n","                cur['min_time'] = info.get(\"time\")\n","                cur['min_winds'] = {wc: info.get(wc) for wc in WIND_COLS}\n","\n","# ========== Finalize: build JSON from agg ==========\n","for serial in stations:\n","    # Ensure all dates exist in output, even if never observed (count=0, others None)\n","    for day_str in date_strs:\n","        day_out = {}\n","        for key in metrics_to_process.keys():\n","            it = agg.get(serial, {}).get(day_str, {}).get(key, None)\n","            if it is None:\n","                # No observation at all for this metric/date\n","                day_out[f\"{key}_count\"] = 0\n","                day_out[f\"{key}_max\"] = None\n","                day_out[f\"{key}_max_time\"] = None\n","                day_out[f\"{key}_min\"] = None\n","                day_out[f\"{key}_min_time\"] = None\n","                day_out[f\"{key}_mean\"] = None\n","                for wc in WIND_COLS:\n","                    day_out[f\"{key}_max_{wc}\"] = None\n","                    day_out[f\"{key}_min_{wc}\"] = None\n","            else:\n","                # Mean from accumulated sum/count\n","                if it['n_mean'] > 0:\n","                    mean_val = it['sum_mean'] / it['n_mean']\n","                else:\n","                    mean_val = None\n","                day_out[f\"{key}_count\"] = int(it['count'])\n","                day_out[f\"{key}_max\"] = None if it['max_val'] is None else float(it['max_val'])\n","                day_out[f\"{key}_max_time\"] = it['max_time']\n","                day_out[f\"{key}_min\"] = None if it['min_val'] is None else float(it['min_val'])\n","                day_out[f\"{key}_min_time\"] = it['min_time']\n","                day_out[f\"{key}_mean\"] = None if mean_val is None else float(mean_val)\n","                # Wind snapshots at extrema\n","                for wc in WIND_COLS:\n","                    day_out[f\"{key}_max_{wc}\"] = it['max_winds'].get(wc) if it['max_winds'] else None\n","                    day_out[f\"{key}_min_{wc}\"] = it['min_winds'].get(wc) if it['min_winds'] else None\n","\n","        final_result[serial][\"data\"][day_str] = day_out"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":512},"id":"rXcoZMU0RHBH","executionInfo":{"status":"error","timestamp":1754818378033,"user_tz":-540,"elapsed":7752,"user":{"displayName":"geumjin99","userId":"13917999733299572466"}},"outputId":"6c39a6c0-c4b1-461d-cf9c-c5d9e0b8069c"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["Files:   2%|▏         | 1/53 [00:07<06:28,  7.48s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2761754935.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweekly_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mdfw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"시리얼\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"측정시간\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2761754935.py\u001b[0m in \u001b[0;36mread_csv_safe\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"\"\"Robust CSV reader handling utf-8-sig/cp949 and bad lines; cleans header.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8-sig\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cp949\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._maybe_upcast\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/multiarray.py\u001b[0m in \u001b[0;36mputmask\u001b[0;34m(a, mask, values)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_from_c_func_and_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_multiarray_umath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     \"\"\"\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# ========== Save JSON ==========\n","os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)\n","with open(OUTPUT_JSON, \"w\", encoding=\"utf-8-sig\") as f:\n","    json.dump(final_result, f, ensure_ascii=False, indent=2)\n","\n","print(f\"✅ JSON saved to {OUTPUT_JSON}\")"],"metadata":{"id":"OtLXlQ-rRIFV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from datetime import datetime, timedelta\n","import numpy as np\n","\n","# ===== Parameters =====\n","INPUT_JSON = \"/content/drive/Othercomputers/我的 Mac/pm_stats/results/summary.json\"\n","DATE_START = \"2024-01-01\"\n","DATE_END   = \"2024-01-31\"\n","SELECTED_METRIC = \"TEMP\"   # e.g., \"TEMP\", \"NO2\", \"CO\", \"SO2\", \"NH3\", \"H2S\", \"O3\"\n","\n","# ===== Read JSON =====\n","with open(INPUT_JSON, \"r\", encoding=\"utf-8-sig\") as f:\n","    data = json.load(f)\n","\n","# Normalize metric key (JSON keys are uppercase prefixes like NO2, TEMP)\n","SELECTED_METRIC = SELECTED_METRIC.strip().upper()\n","count_key = f\"{SELECTED_METRIC}_count\"\n","\n","# ===== Generate complete date list =====\n","start_dt = datetime.strptime(DATE_START, \"%Y-%m-%d\")\n","end_dt = datetime.strptime(DATE_END, \"%Y-%m-%d\")\n","all_days = [(start_dt + timedelta(days=i)).strftime(\"%Y-%m-%d\")\n","            for i in range((end_dt - start_dt).days + 1)]\n","total_days = len(all_days)\n","\n","# ===== Calculate coverage rate (per station) =====\n","coverage_list = []  # [(serial, valid_days, coverage_rate), ...]\n","metric_seen_somewhere = False\n","\n","for serial, payload in data.items():\n","    day_map = payload.get(\"data\", {})\n","    valid_days = 0\n","    for d in all_days:\n","        rec = day_map.get(d)\n","        if not rec:\n","            continue\n","        # Count this day only if SELECTED_METRIC has count > 0\n","        if count_key in rec:\n","            metric_seen_somewhere = True\n","            if (rec.get(count_key) or 0) > 0:\n","                valid_days += 1\n","        else:\n","            # If the metric-specific key is missing entirely on this day, treat as no data\n","            pass\n","    rate = valid_days / total_days if total_days else 0.0\n","    coverage_list.append((serial, valid_days, rate))\n","\n","# ===== Output results =====\n","rates = [r for _, _, r in coverage_list]\n","print(f\"[Metric] {SELECTED_METRIC}\")\n","print(f\"Total stations: {len(coverage_list)}\")\n","print(f\"Total days: {total_days}\")\n","print(f\"Average coverage rate: {np.mean(rates)*100:.2f}%\")\n","print(f\"Median coverage rate: {np.median(rates)*100:.2f}%\")\n","print(f\"Lowest coverage rate: {min(rates)*100:.2f}%\")\n","print(f\"Highest coverage rate: {max(rates)*100:.2f}%\")\n","\n","print(\"\\nExample (first 10 stations):\")\n","for serial, valid, rate in coverage_list[:10]:\n","    print(f\"{serial}: {valid}/{total_days} days, coverage rate {rate*100:.2f}%\")\n","\n","if not metric_seen_somewhere:\n","    print(f\"\\n⚠️ The metric '{SELECTED_METRIC}' was not found in any day's record \"\n","          f\"(no '{count_key}' keys). Double-check your metric name and JSON content.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uuC6x04eRjyG","executionInfo":{"status":"ok","timestamp":1754817697750,"user_tz":-540,"elapsed":907,"user":{"displayName":"geumjin99","userId":"13917999733299572466"}},"outputId":"12321f10-249a-4ad0-93b8-77864f2ca28b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[Metric] TEMP\n","Total stations: 1130\n","Total days: 31\n","Average coverage rate: 83.21%\n","Median coverage rate: 96.77%\n","Lowest coverage rate: 0.00%\n","Highest coverage rate: 96.77%\n","\n","Example (first 10 stations):\n","OC3CL200011: 0/31 days, coverage rate 0.00%\n","OC3CL200020: 30/31 days, coverage rate 96.77%\n","OC3CL200026: 30/31 days, coverage rate 96.77%\n","OC3CL200027: 30/31 days, coverage rate 96.77%\n","OC3CL200012: 30/31 days, coverage rate 96.77%\n","OC3CL200017: 0/31 days, coverage rate 0.00%\n","OC3CL200032: 30/31 days, coverage rate 96.77%\n","OC3CL200030: 30/31 days, coverage rate 96.77%\n","OC3CL200022: 30/31 days, coverage rate 96.77%\n","OC3CL200025: 30/31 days, coverage rate 96.77%\n"]}]}]}